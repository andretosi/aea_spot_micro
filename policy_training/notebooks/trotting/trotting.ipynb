{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb68d890-186f-43ef-8c85-3bf3e20712fe",
   "metadata": {},
   "source": [
    "# Trotting behaviour\n",
    "The purpose of this notebook is to teach how to move to a policy that has learned to _stand_. \n",
    "It is the first step towards walking, and usually manifests with a policy that moves around dragging its feet (-> shuffling around) or that moves with little rythmic jumps (-> trotting).\n",
    "In this case we have obtained the latter result, that we hope to refine into proper walking with the next notebook.\n",
    "Again, we will provide a training and an evaluation section, along with the reward function and the \"stand\" policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea23fef0-3456-4a40-bdb2-be99c52fc965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Start from the current working directory (where notebook is)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Go two levels up (to the \"grandparent\")\n",
    "grandparent_dir = os.path.abspath(os.path.join(cwd, \"..\", \"..\"))\n",
    "\n",
    "# Add to sys.path if not already there\n",
    "if grandparent_dir not in sys.path:\n",
    "    sys.path.insert(0, grandparent_dir)\n",
    "\n",
    "from SpotmicroEnv import SpotmicroEnv\n",
    "from reward_function import reward_function, RewardState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7bb15c-4153-4613-839a-0b1debcc3423",
   "metadata": {},
   "source": [
    "# Training\n",
    "The training process is functionally the same we have encountered in the \"standing\" notebook. The reward funciton for this purpose, however, is much more complex than the one we havve seen in that notebook, since the final goal is much more complex now.\n",
    "\n",
    "## Reward function\n",
    "The reward function for this notebook was designed with the goal of moving a policy that wants to stand still in mind. \n",
    "We want to reward following specific directions, but we need to reward any motion above everything else. For this reason, the only reward in the reward function is that deriving from the optimal tracking of the reference velocity. All other components are penalties, whose weight add up to sligh0lty less than the weight of the reward. This is done to avoid any \"suicide\" from the robot, that might find a local optimum in \"cutting all penalties short by terminating early\".\n",
    "\n",
    "The penalties ensure that the robot:\n",
    "- Follows reference angular velocity\n",
    "- Stays at a proper height and with a proper posture\n",
    "- Does not drift from the target direction\n",
    "- Uses as much small action as possible\n",
    "- Stays as much close to the homing position as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "667ac4c2-12db-432a-98b4-7602eb6e40d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/aea_spot_micro/policy_training/SpotmicroEnv.py:116: UserWarning: File 'trot.pkl' already exists and will be overwritten.\n",
      "  warnings.warn(f\"File '{self._dest_save}' already exists and will be overwritten.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# ========= CONFIG ==========\n",
    "TOTAL_STEPS = 13_000_000\n",
    "run = \"trot\"\n",
    "base=\"stand\"\n",
    "\n",
    "log_dir = f\"./logs/{run}\"\n",
    "\n",
    "def clipped_linear_schedule(initial_value, min_value=1e-5):\n",
    "    def schedule(progress_remaining):\n",
    "        return max(progress_remaining * initial_value, min_value)\n",
    "    return schedule\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=TOTAL_STEPS // 13,\n",
    "    save_path=f\"{run}_checkpoints\",\n",
    "    name_prefix=f\"ppo_{run}\"\n",
    ")\n",
    "\n",
    "# ========= ENV ==========\n",
    "env = SpotmicroEnv(\n",
    "    use_gui=False,\n",
    "    reward_fn=reward_function, \n",
    "    reward_state=RewardState(),\n",
    "    src_save_file=f\"{base}.pkl\",\n",
    "    dest_save_file=f\"{run}.pkl\"\n",
    ")\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# ========= MODEL ==========\n",
    "model = PPO.load(f\"ppo_{base}\")\n",
    "model.set_env(env)\n",
    "model.tensorboard_log = log_dir\n",
    "\n",
    "# Custom logger: ONLY csv + tensorboard (no stdout table)\n",
    "new_logger = configure(log_dir, [\"csv\", \"tensorboard\"])\n",
    "model.set_logger(new_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b907fb-8983-4756-8ed2-be26f4ad2471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c04f893947e40ba\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c04f893947e40ba\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79693176-28e2-4166-b359-8e5f1d2a36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(\n",
    "    total_timesteps=TOTAL_STEPS,\n",
    "    reset_num_timesteps=False,\n",
    "    callback=checkpoint_callback\n",
    ")\n",
    "model.save(f\"ppo_{run}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353a413-446d-4b1b-8fbc-0ee4147e1965",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "The evaluation process is exaclty the same as shown in the walking notebook.\n",
    "\n",
    "## Results\n",
    "The resulting policy exhibit promising behaviour, that closely resembles walking ans only has to be refined and made robust ahainst rougher terrains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeffb067-676c-41da-b3d7-e2df2e32072e",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "Cannot load URDF file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m policy = \u001b[33m\"\u001b[39m\u001b[33mtrot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m env = \u001b[43mSpotmicroEnv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_gui\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRewardState\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_save_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpolicy\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m obs, _ = env.reset()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# === Load model ===\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spot_micro_project/aea_spot_micro/policy_training/SpotmicroEnv.py:240\u001b[39m, in \u001b[36mSpotmicroEnv.__init__\u001b[39m\u001b[34m(self, envConfig, agentConfig, terrainConfig, use_gui, reward_fn, reward_state, dest_save_file, src_save_file, writer)\u001b[39m\n\u001b[32m    229\u001b[39m pybullet.changeDynamics(\n\u001b[32m    230\u001b[39m     bodyUniqueId=\u001b[38;5;28mself\u001b[39m._terrain.terrain_id,\n\u001b[32m    231\u001b[39m     linkIndex=-\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     physicsClientId=\u001b[38;5;28mself\u001b[39m.physics_client\n\u001b[32m    237\u001b[39m )\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m#Initialize the agent object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28mself\u001b[39m._agent = \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mphysics_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43magentConfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ACT_SPACE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn_height\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[38;5;28mself\u001b[39m._dest_save = dest_save_file\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dest_save \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spot_micro_project/aea_spot_micro/policy_training/Agent.py:172\u001b[39m, in \u001b[36mAgent.__init__\u001b[39m\u001b[34m(self, physics_client, config, action_space_size, spawn_height)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28mself\u001b[39m._joint_history = deque(maxlen=\u001b[38;5;28mself\u001b[39m.config.joint_history_maxlen) \u001b[38;5;66;03m# It will hold tuples with np.ndarray of joint_positions and joint_velocities\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# --- Load URDF ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28mself\u001b[39m._robot_id = \u001b[43mpybullet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadURDF\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspotmicroai.urdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbasePosition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbase_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaseOrientation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbase_orientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mphysicsClientId\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mphysics_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# --- Joints ---\u001b[39;00m\n\u001b[32m    180\u001b[39m motor_joints = []\n",
      "\u001b[31merror\u001b[39m: Cannot load URDF file."
     ]
    }
   ],
   "source": [
    "policy = \"trot\"\n",
    "\n",
    "env = SpotmicroEnv(\n",
    "    use_gui=True, \n",
    "    reward_fn=reward_function,\n",
    "    reward_state=RewardState(),\n",
    "    src_save_file=f\"{policy}.pkl\"\n",
    "    )\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# === Load model ===\n",
    "model = PPO.load(f\"ppo_{policy}\", device = 'cpu')\n",
    "#model = PPO.load(f\"{policy}_checkpoints/ppo_{policy}_16001216_steps\")\n",
    "\n",
    "# === Run rollout ===\n",
    "for _ in range(3001):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"Terminated\")\n",
    "        env.plot_reward_components()\n",
    "        obs, _ = env.reset()\n",
    "    time.sleep(1/60)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a850d3-218a-41f0-9b56-446484d5bdec",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "The next step is to refine this policy and make the \"step\" movement more natural and more robust. This will be done by modelling a rough terrain to train the robot on, so that the policy has to lift the leg more, and be more cautious overall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
