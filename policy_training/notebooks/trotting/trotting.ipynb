{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb68d890-186f-43ef-8c85-3bf3e20712fe",
   "metadata": {},
   "source": [
    "# Trotting behaviour\n",
    "The purpose of this notebook is to teach how to move to a policy that has learned to _stand_. \n",
    "It is the first step towards walking, and usually manifests with a policy that moves around dragging its feet (-> shuffling around) or that moves with little rythmic jumps (-> trotting).\n",
    "In this case we have obtained the latter result, that we hope to refine into proper walking with the next notebook.\n",
    "Again, we will provide a training and an evaluation section, along with the reward function and the \"stand\" policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea23fef0-3456-4a40-bdb2-be99c52fc965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Apr  4 2025 18:56:19\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Start from the current working directory (where notebook is)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Go two levels up (to the \"grandparent\")\n",
    "grandparent_dir = os.path.abspath(os.path.join(cwd, \"..\", \"..\"))\n",
    "\n",
    "# Add to sys.path if not already there\n",
    "if grandparent_dir not in sys.path:\n",
    "    sys.path.insert(0, grandparent_dir)\n",
    "\n",
    "from SpotmicroEnv import SpotmicroEnv\n",
    "from reward_function import reward_function, RewardState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7bb15c-4153-4613-839a-0b1debcc3423",
   "metadata": {},
   "source": [
    "# Training\n",
    "The training process is functionally the same we have encountered in the \"standing\" notebook. The reward funciton for this purpose, however, is much more complex than the one we havve seen in that notebook, since the final goal is much more complex now.\n",
    "\n",
    "## Reward function\n",
    "The reward function for this notebook was designed with the goal of moving a policy that wants to stand still in mind. \n",
    "We want to reward following specific directions, but we need to reward any motion above everything else. For this reason, the only reward in the reward function is that deriving from the optimal tracking of the reference velocity. All other components are penalties, whose weight add up to sligh0lty less than the weight of the reward. This is done to avoid any \"suicide\" from the robot, that might find a local optimum in \"cutting all penalties short by terminating early\".\n",
    "\n",
    "The penalties ensure that the robot:\n",
    "- Follows reference angular velocity\n",
    "- Stays at a proper height and with a proper posture\n",
    "- Does not drift from the target direction\n",
    "- Uses as much small action as possible\n",
    "- Stays as much close to the homing position as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "667ac4c2-12db-432a-98b4-7602eb6e40d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/aea_spot_micro/policy_training/SpotmicroEnv.py:116: UserWarning: File 'trot.pkl' already exists and will be overwritten.\n",
      "  warnings.warn(f\"File '{self._dest_save}' already exists and will be overwritten.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# ========= CONFIG ==========\n",
    "TOTAL_STEPS = 13_000_000\n",
    "run = \"trot\"\n",
    "base=\"stand\"\n",
    "\n",
    "log_dir = f\"./logs/{run}\"\n",
    "\n",
    "def clipped_linear_schedule(initial_value, min_value=1e-5):\n",
    "    def schedule(progress_remaining):\n",
    "        return max(progress_remaining * initial_value, min_value)\n",
    "    return schedule\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=TOTAL_STEPS // 13,\n",
    "    save_path=f\"{run}_checkpoints\",\n",
    "    name_prefix=f\"ppo_{run}\"\n",
    ")\n",
    "\n",
    "# ========= ENV ==========\n",
    "env = SpotmicroEnv(\n",
    "    use_gui=False,\n",
    "    reward_fn=reward_function, \n",
    "    reward_state=RewardState(),\n",
    "    src_save_file=f\"{base}.pkl\",\n",
    "    dest_save_file=f\"{run}.pkl\"\n",
    ")\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# ========= MODEL ==========\n",
    "model = PPO.load(f\"ppo_{base}\")\n",
    "model.set_env(env)\n",
    "model.tensorboard_log = log_dir\n",
    "\n",
    "# Custom logger: ONLY csv + tensorboard (no stdout table)\n",
    "new_logger = configure(log_dir, [\"csv\", \"tensorboard\"])\n",
    "model.set_logger(new_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b907fb-8983-4756-8ed2-be26f4ad2471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c04f893947e40ba\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c04f893947e40ba\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79693176-28e2-4166-b359-8e5f1d2a36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(\n",
    "    total_timesteps=TOTAL_STEPS,\n",
    "    reset_num_timesteps=False,\n",
    "    callback=checkpoint_callback\n",
    ")\n",
    "model.save(f\"ppo_{run}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353a413-446d-4b1b-8fbc-0ee4147e1965",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "The evaluation process is exaclty the same as shown in the walking notebook.\n",
    "\n",
    "## Results\n",
    "The resulting policy exhibit promising behaviour, that closely resembles walking ans only has to be refined and made robust ahainst rougher terrains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeffb067-676c-41da-b3d7-e2df2e32072e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Num of steps per rollout: 2048\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env.max_episode_len + \u001b[32m1\u001b[39m):\n\u001b[32m     18\u001b[39m     action, _ = model.predict(obs, deterministic=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     obs, reward, terminated, truncated, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[32m     22\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTerminated\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/policy_training/SpotmicroEnv.py:233\u001b[39m, in \u001b[36mSpotmicroEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: np.ndarray) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    218\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m    Method exposed and used bby SB3 to execute one time step within the environment.\u001b[39;00m\n\u001b[32m    220\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    230\u001b[39m \u001b[33;03m            - info (dict): Contains auxiliary diagnostic information.\u001b[39;00m\n\u001b[32m    231\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m     observation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28mself\u001b[39m._episode_step_counter += \u001b[32m1\u001b[39m \u001b[38;5;66;03m#updates the step counter (used to check against timeouts)\u001b[39;00m\n\u001b[32m    235\u001b[39m     reward, reward_info = \u001b[38;5;28mself\u001b[39m._calculate_reward(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/policy_training/SpotmicroEnv.py:295\u001b[39m, in \u001b[36mSpotmicroEnv._step_simulation\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._terrain.tilt_plane()\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m._SIM_FREQUENCY // \u001b[38;5;28mself\u001b[39m._CONTROL_FREQUENCY):\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[43mpybullet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstepSimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_gui:\n\u001b[32m    297\u001b[39m         time.sleep(\u001b[32m1\u001b[39m/\u001b[32m70.\u001b[39m) \u001b[38;5;66;03m# MAGIC NUMBER, MAKES THE SIMULATION LOOK REAL-TIME (not slow, not too fast)\u001b[39;00m\n",
      "\u001b[31merror\u001b[39m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "policy = \"trot3\"\n",
    "\n",
    "env = SpotmicroEnv(\n",
    "    use_gui=True, \n",
    "    reward_fn=reward_function,\n",
    "    reward_state=RewardState(),\n",
    "    src_save_file=f\"{policy}.pkl\"\n",
    "    )\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# === Load model ===\n",
    "model = PPO.load(f\"ppo_{policy}\")\n",
    "#model = PPO.load(f\"{policy}_checkpoints/ppo_{policy}_8000896_steps\")\n",
    "print(f\"\\nNum of steps per rollout: {model.n_steps}\")\n",
    "s0 = env.num_steps\n",
    "# === Run rollout ===\n",
    "for _ in range(env.max_episode_len + 1):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"Terminated\")\n",
    "        env.plot_reward_components()\n",
    "        obs, _ = env.reset()\n",
    "    \n",
    "    time.sleep(1/60)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a850d3-218a-41f0-9b56-446484d5bdec",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "The next step is to refine this policy and make the \"step\" movement more natural and more robust. This will be done by modelling a rough terrain to train the robot on, so that the policy has to lift the leg more, and be more cautious overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b1120-b301-4732-ad2e-9f27ff1ee36b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
