{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e09453-83ba-40a2-b3bb-a7cf2404c8b4",
   "metadata": {},
   "source": [
    "# Standing Behaviour\n",
    "> This directory contains all the building blocks required to achieve the most basic learnable behaviour: standing still.  \n",
    "> \n",
    "> ⚠️ **IMPORTANT WARNING** ⚠️  \n",
    "> This notebook, together with the contents of the entire directory, is intended to serve as a tutorial and documentation of the results achieved so far.  \n",
    "> It should **NOT** be modified.  \n",
    "> \n",
    "> __To experiment with this task, please create a copy of this directory and make your changes there.__\n",
    "\n",
    "## Contents\n",
    "This directory includes:\n",
    "- This notebook  \n",
    "- The file `reward_function.py`, which defines a reward function tailored for achieving standing behaviour with deep reinforcement learning  \n",
    "- `ppo_stand.zip`, a pretrained policy that demonstrates standing behaviour  \n",
    "- Three config files (`env`, `agent`, and `terrain` configs) that specify all parameters for this experiment. In particular, note the definition of the **home positions** of each joint in `agentConfig.yaml`, since these determine the pose the robot assumes when resting  \n",
    "- `stand.pkl`, a state file produced at the end of a training session. It is used by the environment to store necessary data such as the total number of training steps  \n",
    "\n",
    "**Note:** The file `SpotmicroEnv.py` defines the custom training environment. For the notebook to work, it must be located in the *grandparent directory* of this one.\n",
    "\n",
    "## Use\n",
    "This notebook, along with the directory contents, demonstrates the basic workflow of training and testing a simple policy.  \n",
    "It also serves as documentation, since this policy will be used as a foundation for later experiments.\n",
    "\n",
    "The notebook is organized as follows:\n",
    "- The first cell (imports) must be executed every time; it loads almost all dependencies  \n",
    "- The first section covers training a custom policy.  \n",
    "  - To experiment, copy this directory elsewhere and adjust the reward function or hyperparameters there  \n",
    "  - Otherwise, you can stick to the provided base policy and skip directly to testing  \n",
    "- The final section covers testing: exploring the results and analyzing the learned policy  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b47e99-3d92-4f7a-b231-00dbf14f381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Apr  4 2025 18:56:19\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Start from the current working directory (where notebook is)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "# Go two levels up (to the \"grandparent\")\n",
    "grandparent_dir = os.path.abspath(os.path.join(cwd, \"..\", \"..\"))\n",
    "\n",
    "# Add to sys.path if not already there\n",
    "if grandparent_dir not in sys.path:\n",
    "    sys.path.insert(0, grandparent_dir)\n",
    "\n",
    "from SpotmicroEnv import SpotmicroEnv\n",
    "from reward_function import reward_function, RewardState\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a57400-cb70-4ea5-a613-4347a0058eb8",
   "metadata": {},
   "source": [
    "# Training\n",
    "The following cells will launch a training session for a policy\n",
    "The first cell will only load the necessary assets and set everything up, while the second one will load tensorboard to visualize useful data about the ongoing training.\n",
    "\n",
    "> NOTE: this directory contains a pre-trained policy \"stand\". You can skip the training cells if you don't need anything specific, and jump to the testing section\n",
    "\n",
    "## Parameters\n",
    "- You can set the name of the policy being trained by assignin it to the \"run\" variable.\n",
    "- You can set the number of checkpoints that will be saved, changing the number within \"checkpoint_callback\"\n",
    "- You can adjust learning rate, entropy coefficient, clip range and the rest of the hyperparameters on the last fiew lines of the notebook\n",
    "\n",
    "## The rewad function\n",
    "The reward function is defined in another file, and is crucial to the success of the experiment. In this case, just two metrics are sufficient to define the desired behaviour:\n",
    "- The agent is given a reward inversely proportional to the average magnitude of each action: the closer the action average is to 0 (and to the homing position) the higher the reward\n",
    "- The agent is given a penalty proportional to the mean squared percentual effort applied to the joints (the effort applied to each joint is normalized by the highest value of the torque allowed for the given joint -> percentual effort)\n",
    "\n",
    "These 2 metrics \"teach\" the robot to stand in a comfortable position, without any jittering or movement, using the least amount of energy possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40900bb-0801-4791-89c1-439a8190c2fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SpotmicroEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     15\u001b[39m checkpoint_callback = CheckpointCallback(\n\u001b[32m     16\u001b[39m     save_freq=TOTAL_STEPS // \u001b[32m5\u001b[39m,\n\u001b[32m     17\u001b[39m     save_path=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_checkpoints\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     name_prefix=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mppo_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# ========= ENV ==========\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m env = \u001b[43mSpotmicroEnv\u001b[49m(\n\u001b[32m     23\u001b[39m     use_gui=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     24\u001b[39m     reward_fn=reward_function, \n\u001b[32m     25\u001b[39m     reward_state=RewardState(), \n\u001b[32m     26\u001b[39m     dest_save_file=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pkl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m )\n\u001b[32m     28\u001b[39m check_env(env, warn=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# ========= MODEL ==========\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'SpotmicroEnv' is not defined"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# ========= CONFIG ==========\n",
    "TOTAL_STEPS = 3_000_000\n",
    "run = \"stand3\"\n",
    "log_dir = f\"./logs/{run}\"\n",
    "\n",
    "def clipped_linear_schedule(initial_value, min_value=1e-5):\n",
    "    def schedule(progress_remaining):\n",
    "        return max(progress_remaining * initial_value, min_value)\n",
    "    return schedule\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=TOTAL_STEPS // 5,\n",
    "    save_path=f\"{run}_checkpoints\",\n",
    "    name_prefix=f\"ppo_{run}\"\n",
    ")\n",
    "\n",
    "# ========= ENV ==========\n",
    "env = SpotmicroEnv(\n",
    "    use_gui=False,\n",
    "    reward_fn=reward_function, \n",
    "    reward_state=RewardState(), \n",
    "    dest_save_file=f\"{run}.pkl\"\n",
    ")\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# ========= MODEL ==========\n",
    "model = PPO(\n",
    "    \"MlpPolicy\", \n",
    "    env,\n",
    "    verbose=0,   # no default printouts\n",
    "    learning_rate=clipped_linear_schedule(3e-4),\n",
    "    ent_coef=0.001,\n",
    "    clip_range=0.1,\n",
    "    tensorboard_log=log_dir,\n",
    ")\n",
    "\n",
    "# Custom logger: ONLY csv + tensorboard (no stdout table)\n",
    "new_logger = configure(log_dir, [\"csv\", \"tensorboard\"])\n",
    "model.set_logger(new_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2474099-7ae6-42dc-88ae-4830f0ce9ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b46fd5363de08298\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b46fd5363de08298\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dfac21d-4a2e-408e-9331-f5fd3b9cb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= TRAIN ==========\n",
    "model.learn(\n",
    "    total_timesteps=TOTAL_STEPS,\n",
    "    reset_num_timesteps=False,\n",
    "    callback=checkpoint_callback\n",
    ")\n",
    "model.save(f\"ppo_{run}\")\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa9b9cc4-083a-4059-b544-7d9fe62d21bc",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "The following cells allow to test the policy we have just trained. All we have to do is assign the name of the policy we have trained to the \"policy\" variable.\n",
    "You can then run the second to last cell any times you want, and observe a single episode until termination. When you are done, execute the last cell to clean everything up.\n",
    "\n",
    "> If in any case there seems to be some sort of weird error, try to reload the kernel of this jupyter notebook first (pybullet is kind of messy in its cleanup phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaecb925-6799-46db-8d94-97e94dd631cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m action, _ = model.predict(obs, deterministic=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#action = np.array([j.from_position_to_action(hp) for j, hp in zip(env.agent.motor_joints, env.agent.homing_positions)])\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m obs, reward, terminated, truncated, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m time.sleep(\u001b[32m1\u001b[39m/\u001b[32m60.\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/policy_training/SpotmicroEnv.py:233\u001b[39m, in \u001b[36mSpotmicroEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: np.ndarray) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    218\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m    Method exposed and used bby SB3 to execute one time step within the environment.\u001b[39;00m\n\u001b[32m    220\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    230\u001b[39m \u001b[33;03m            - info (dict): Contains auxiliary diagnostic information.\u001b[39;00m\n\u001b[32m    231\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m     observation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28mself\u001b[39m._episode_step_counter += \u001b[32m1\u001b[39m \u001b[38;5;66;03m#updates the step counter (used to check against timeouts)\u001b[39;00m\n\u001b[32m    235\u001b[39m     reward, reward_info = \u001b[38;5;28mself\u001b[39m._calculate_reward(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/policy_training/SpotmicroEnv.py:295\u001b[39m, in \u001b[36mSpotmicroEnv._step_simulation\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._terrain.tilt_plane()\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m._SIM_FREQUENCY // \u001b[38;5;28mself\u001b[39m._CONTROL_FREQUENCY):\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[43mpybullet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstepSimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_gui:\n\u001b[32m    297\u001b[39m         time.sleep(\u001b[32m1\u001b[39m/\u001b[32m70.\u001b[39m) \u001b[38;5;66;03m# MAGIC NUMBER, MAKES THE SIMULATION LOOK REAL-TIME (not slow, not too fast)\u001b[39;00m\n",
      "\u001b[31merror\u001b[39m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "policy = \"stand\"\n",
    "\n",
    "env = SpotmicroEnv(\n",
    "    use_gui=True, \n",
    "    reward_fn=reward_function,\n",
    "    reward_state=RewardState(),\n",
    "    src_save_file=f\"{policy}.pkl\"\n",
    "    )\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# === Load model ===\n",
    "model = PPO.load(f\"ppo_{policy}\")\n",
    "#model = PPO.load(f\"{policy}_checkpoints/ppo_{policy}_3000000_steps\")\n",
    "base_steps = env.num_steps\n",
    "\n",
    "t0 = time.time()\n",
    "for _ in range(3001):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    #action = np.array([j.from_position_to_action(hp) for j, hp in zip(env.agent.motor_joints, env.agent.homing_positions)])\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    time.sleep(1/60.)\n",
    "    if terminated or truncated:\n",
    "        print(\"Terminated\")\n",
    "        env.plot_reward_components()  # plot per episode\n",
    "        obs, _ = env.reset()\n",
    "        print(f\"Num steps: {env.num_steps - base_steps}\")\n",
    "        break\n",
    "    \n",
    "t1 = time.time()\n",
    "print(f\"Elapsed real time: {t1-t0}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91375798-c6f6-4895-b7ca-c4a458e7b9d7",
   "metadata": {},
   "source": [
    "# What is next?\n",
    "The next important step towards walking is convincing the policy to move at all. It is not a trivial task, since it involves designing a reward function that makes moving more attractive than both standing still and falling flat. This task is explored in the notebook inside the \"shuffling\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb95bd-07b1-46fb-91a3-3922f298f888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
