{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e09453-83ba-40a2-b3bb-a7cf2404c8b4",
   "metadata": {},
   "source": [
    "# Standing Behaviour\n",
    "> This directory contains all the building blocks required to achieve the most basic learnable behaviour: standing still.  \n",
    "> \n",
    "> ⚠️ **IMPORTANT WARNING** ⚠️  \n",
    "> This notebook, together with the contents of the entire directory, is intended to serve as a tutorial and documentation of the results achieved so far.  \n",
    "> It should **NOT** be modified.  \n",
    "> \n",
    "> __To experiment with this task, please create a copy of this directory and make your changes there.__\n",
    "\n",
    "## Contents\n",
    "This directory includes:\n",
    "- This notebook  \n",
    "- The file `reward_function.py`, which defines a reward function tailored for achieving standing behaviour with deep reinforcement learning  \n",
    "- `ppo_stand.zip`, a pretrained policy that demonstrates standing behaviour  \n",
    "- Three config files (`env`, `agent`, and `terrain` configs) that specify all parameters for this experiment. In particular, note the definition of the **home positions** of each joint in `agentConfig.yaml`, since these determine the pose the robot assumes when resting  \n",
    "- `stand.pkl`, a state file produced at the end of a training session. It is used by the environment to store necessary data such as the total number of training steps  \n",
    "\n",
    "**Note:** The file `SpotmicroEnv.py` defines the custom training environment. For the notebook to work, it must be located in the *grandparent directory* of this one.\n",
    "\n",
    "## Use\n",
    "This notebook, along with the directory contents, demonstrates the basic workflow of training and testing a simple policy.  \n",
    "It also serves as documentation, since this policy will be used as a foundation for later experiments.\n",
    "\n",
    "The notebook is organized as follows:\n",
    "- The first cell (imports) must be executed every time; it loads almost all dependencies  \n",
    "- The first section covers training a custom policy.  \n",
    "  - To experiment, copy this directory elsewhere and adjust the reward function or hyperparameters there  \n",
    "  - Otherwise, you can stick to the provided base policy and skip directly to testing  \n",
    "- The final section covers testing: exploring the results and analyzing the learned policy  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b47e99-3d92-4f7a-b231-00dbf14f381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Apr  4 2025 18:56:19\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Start from the current working directory (where notebook is)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Go two levels up (to the \"grandparent\")\n",
    "grandparent_dir = os.path.abspath(os.path.join(cwd, \"..\", \"..\"))\n",
    "\n",
    "# Add to sys.path if not already there\n",
    "if grandparent_dir not in sys.path:\n",
    "    sys.path.insert(0, grandparent_dir)\n",
    "\n",
    "from SpotmicroEnv import SpotmicroEnv\n",
    "from reward_function import reward_function, RewardState\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a57400-cb70-4ea5-a613-4347a0058eb8",
   "metadata": {},
   "source": [
    "# Training\n",
    "The following cells will launch a training session for a policy\n",
    "The first cell will only load the necessary assets and set everything up, while the second one will load tensorboard to visualize useful data about the ongoing training.\n",
    "\n",
    "> NOTE: this directory contains a pre-trained policy \"stand\". You can skip the training cells if you don't need anything specific, and jump to the testing section\n",
    "\n",
    "## Parameters\n",
    "- You can set the name of the policy being trained by assignin it to the \"run\" variable.\n",
    "- You can set the number of checkpoints that will be saved, changing the number within \"checkpoint_callback\"\n",
    "- You can adjust learning rate, entropy coefficient, clip range and the rest of the hyperparameters on the last fiew lines of the notebook\n",
    "\n",
    "## The rewad function\n",
    "The reward function is defined in another file, and is crucial to the success of the experiment. In this case, I have defined 5 different rewards/penalties to define a good standing behaviour:\n",
    "- _Uprightness_: this metric should measure the posture of the gait, and should encourage it to stand upright. It is measured through roll and pitch\n",
    "- _Height_: the closer the agent is to a target height set by the user, the bigger the reward it receives\n",
    "- _Vertical velocity penalty_: any sudden and fast movement on the z-axis is heavily penalized, to encourage stillness\n",
    "- _Joint deviation penalty_: the more the position of each joint strays from a set position (homing positions), the heavier the penalty is. This ecnourages the agent to stick to a predefined resting pose\n",
    "-  _Action sparsity reward_: this metric rewards the agent for small actions, and should discourage ample movements\n",
    "\n",
    "Each reward /penalty is linearly combined withh the others with a weight that highlight its importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40900bb-0801-4791-89c1-439a8190c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# ========= CONFIG ==========\n",
    "TOTAL_STEPS = 4_000_000\n",
    "run = \"stand\"\n",
    "log_dir = f\"./logs/{run}\"\n",
    "\n",
    "def clipped_linear_schedule(initial_value, min_value=1e-5):\n",
    "    def schedule(progress_remaining):\n",
    "        return max(progress_remaining * initial_value, min_value)\n",
    "    return schedule\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=TOTAL_STEPS // 10,\n",
    "    save_path=f\"{run}_checkpoints\",\n",
    "    name_prefix=f\"ppo_{run}\"\n",
    ")\n",
    "\n",
    "# ========= ENV ==========\n",
    "env = SpotmicroEnv(\n",
    "    use_gui=False,\n",
    "    reward_fn=reward_function, \n",
    "    reward_state=RewardState(), \n",
    "    dest_save_file=f\"{run}.pkl\"\n",
    ")\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# ========= MODEL ==========\n",
    "model = PPO(\n",
    "    \"MlpPolicy\", \n",
    "    env,\n",
    "    verbose=0,   # no default printouts\n",
    "    learning_rate=clipped_linear_schedule(3e-4),\n",
    "    ent_coef=0.002,\n",
    "    clip_range=0.1,\n",
    "    tensorboard_log=log_dir,\n",
    ")\n",
    "\n",
    "# Custom logger: ONLY csv + tensorboard (no stdout table)\n",
    "new_logger = configure(log_dir, [\"csv\", \"tensorboard\"])\n",
    "model.set_logger(new_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2474099-7ae6-42dc-88ae-4830f0ce9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfac21d-4a2e-408e-9331-f5fd3b9cb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========= TRAIN ==========\n",
    "model.learn(\n",
    "    total_timesteps=TOTAL_STEPS,\n",
    "    reset_num_timesteps=False,\n",
    "    callback=checkpoint_callback\n",
    ")\n",
    "model.save(f\"policies/ppo_{run}\")\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa9b9cc4-083a-4059-b544-7d9fe62d21bc",
   "metadata": {},
   "source": [
    "# Testing\n",
    "The following cells allow to test the policy we have just trained. All we have to do is assign the name of the policy we have trained to the \"policy\" variable.\n",
    "You can then run the second to last cell any times you want, and observe a single episode until termination. When you are done, execute the last cell to clean everything up.\n",
    "\n",
    "> If in any case there seems to be some sort of weird error, try to reload the kernel of this jupyter notebook first (pybullet is kind of messy in its cleanup phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaecb925-6799-46db-8d94-97e94dd631cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generic)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 24.2.8-1ubuntu1~24.04.1\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 24.2.8-1ubuntu1~24.04.1\n",
      "Vendor = AMD\n",
      "Renderer = AMD Radeon Graphics (radeonsi, renoir, LLVM 19.1.1, DRM 3.59, 6.11.0-21-generic)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = AMD\n",
      "ven = AMD\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: front_left_leg_link_cover\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: front_right_leg_link_cover\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: rear_left_leg_link_cover\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: rear_right_leg_link_cover\n",
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=AMD\n",
      "GL_RENDERER=AMD Radeon Graphics (radeonsi, renoir, LLVM 19.1.1, DRM 3.59, 6.11.0-21-generic)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 24.2.8-1ubuntu1~24.04.1\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 24.2.8-1ubuntu1~24.04.1\n",
      "Vendor = AMD\n",
      "Renderer = AMD Radeon Graphics (radeonsi, renoir, LLVM 19.1.1, DRM 3.59, 6.11.0-21-generic)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = AMD\n",
      "ven = AMD\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: front_left_leg_link_cover\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: front_right_leg_link_cover\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frame\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: rear_left_leg_link_cover\n",
      "b3Printf: b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "\n",
      "b3Printf: No inertial data for link, using maLoaded policy and VecNormalize stats\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2000\u001b[39m):  \u001b[38;5;66;03m# run some steps\u001b[39;00m\n\u001b[32m     30\u001b[39m     action, _ = model.predict(obs, deterministic=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     obs, reward, done, info = \u001b[43meval_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     eval_env.render()\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:181\u001b[39m, in \u001b[36mVecNormalize.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m    175\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m    Apply sequence of actions to sequence of environments\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[33;03m    actions -> (observations, rewards, dones)\u001b[39;00m\n\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    where ``dones`` is a boolean vector indicating whether each element is new.\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     obs, rewards, dones, infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvenv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, (np.ndarray, \u001b[38;5;28mdict\u001b[39m))  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[32m    183\u001b[39m     \u001b[38;5;28mself\u001b[39m.old_obs = obs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/policy_training/SpotmicroEnv.py:226\u001b[39m, in \u001b[36mSpotmicroEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:                                                                         \u001b[38;5;66;03m# reuse last action\u001b[39;00m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28mself\u001b[39m._action_counter += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     observation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprevious_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     reward, reward_info = \u001b[38;5;28mself\u001b[39m._calculate_reward(\u001b[38;5;28mself\u001b[39m._agent.previous_action)\n\u001b[32m    229\u001b[39m terminated, term_penalty = \u001b[38;5;28mself\u001b[39m._is_target_state() \u001b[38;5;66;03m# checks wether the agent has fallen or not\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/policy_training/SpotmicroEnv.py:280\u001b[39m, in \u001b[36mSpotmicroEnv._step_simulation\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[33;03mPrivate method that calls the API to execute the given action in PyBullet.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03mIt should sinchronize the state of the agent in the simulation with the state recorded here!\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03mAccepts an action and returns an observation\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# Execute the action in pybullet\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[38;5;28mself\u001b[39m._episode_step_counter += \u001b[32m1\u001b[39m \u001b[38;5;66;03m#updates the step counter (used to check against timeouts)\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._terrain.config.mode == \u001b[33m\"\u001b[39m\u001b[33mtilting\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/policy_training/Agent.py:171\u001b[39m, in \u001b[36mAgent.apply_action\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28mself\u001b[39m._action = action\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, joint \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m._motor_joints):\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[43mpybullet\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetJointMotorControl2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbodyUniqueId\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_robot_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjointIndex\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontrolMode\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpybullet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPOSITION_CONTROL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtargetPosition\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_action_to_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_torque\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31merror\u001b[39m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "policy = \"stand\"\n",
    "\n",
    "# === Build raw env ===\n",
    "def make_env():\n",
    "    return SpotmicroEnv(\n",
    "        use_gui=True,\n",
    "        reward_fn=reward_function,\n",
    "        reward_state=RewardState(),\n",
    "        src_save_file=f\"{policy}.pkl\",\n",
    "    )\n",
    "\n",
    "# DummyVecEnv wrapper\n",
    "raw_env = DummyVecEnv([make_env])\n",
    "\n",
    "# === Load VecNormalize stats ===\n",
    "eval_env = VecNormalize.load(f\"{policy}_vecnormalize.pkl\", raw_env)\n",
    "\n",
    "# Very important: disable training updates during evaluation\n",
    "eval_env.training = False\n",
    "eval_env.norm_reward = False\n",
    "\n",
    "# === Load model ===\n",
    "model = PPO.load(f\"stand_checkpoints/ppo_{policy}_5000000_steps.zip\")\n",
    "\n",
    "print(\"Loaded policy and VecNormalize stats\")\n",
    "\n",
    "# === Run rollout ===\n",
    "obs = eval_env.reset()\n",
    "for _ in range(2000):  # run some steps\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = eval_env.step(action)\n",
    "    eval_env.render()\n",
    "    if done:\n",
    "        obs = eval_env.reset()\n",
    "\n",
    "    time.sleep(1/10.)\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91375798-c6f6-4895-b7ca-c4a458e7b9d7",
   "metadata": {},
   "source": [
    "# What is next?\n",
    "The next important step towards walking is convincing the policy to move at all. It is not a trivial task, since it involves designing a reward function that makes moving more attractive than both standing still and falling flat. This task is explored in the notebook inside the \"shuffling\" directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
