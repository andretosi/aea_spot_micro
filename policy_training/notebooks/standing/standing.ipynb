{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e09453-83ba-40a2-b3bb-a7cf2404c8b4",
   "metadata": {},
   "source": [
    "# Standing Behaviour\n",
    "> This directory contains all the building blocks required to achieve the most basic learnable behaviour: standing still.  \n",
    "> \n",
    "> ⚠️ **IMPORTANT WARNING** ⚠️  \n",
    "> This notebook, together with the contents of the entire directory, is intended to serve as a tutorial and documentation of the results achieved so far.  \n",
    "> It should **NOT** be modified.  \n",
    "> \n",
    "> __To experiment with this task, please create a copy of this directory and make your changes there.__\n",
    "\n",
    "## Contents\n",
    "This directory includes:\n",
    "- This notebook  \n",
    "- The file `reward_function.py`, which defines a reward function tailored for achieving standing behaviour with deep reinforcement learning  \n",
    "- `ppo_stand.zip`, a pretrained policy that demonstrates standing behaviour  \n",
    "- Three config files (`env`, `agent`, and `terrain` configs) that specify all parameters for this experiment. In particular, note the definition of the **home positions** of each joint in `agentConfig.yaml`, since these determine the pose the robot assumes when resting  \n",
    "- `stand.pkl`, a state file produced at the end of a training session. It is used by the environment to store necessary data such as the total number of training steps  \n",
    "\n",
    "**Note:** The file `SpotmicroEnv.py` defines the custom training environment. For the notebook to work, it must be located in the *grandparent directory* of this one.\n",
    "\n",
    "## Use\n",
    "This notebook, along with the directory contents, demonstrates the basic workflow of training and testing a simple policy.  \n",
    "It also serves as documentation, since this policy will be used as a foundation for later experiments.\n",
    "\n",
    "The notebook is organized as follows:\n",
    "- The first cell (imports) must be executed every time; it loads almost all dependencies  \n",
    "- The first section covers training a custom policy.  \n",
    "  - To experiment, copy this directory elsewhere and adjust the reward function or hyperparameters there  \n",
    "  - Otherwise, you can stick to the provided base policy and skip directly to testing  \n",
    "- The final section covers testing: exploring the results and analyzing the learned policy  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b47e99-3d92-4f7a-b231-00dbf14f381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Apr  4 2025 18:56:19\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Start from the current working directory (where notebook is)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Go two levels up (to the \"grandparent\")\n",
    "grandparent_dir = os.path.abspath(os.path.join(cwd, \"..\", \"..\"))\n",
    "\n",
    "# Add to sys.path if not already there\n",
    "if grandparent_dir not in sys.path:\n",
    "    sys.path.insert(0, grandparent_dir)\n",
    "\n",
    "from SpotmicroEnv import SpotmicroEnv\n",
    "from reward_function import reward_function, RewardState\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a57400-cb70-4ea5-a613-4347a0058eb8",
   "metadata": {},
   "source": [
    "# Training\n",
    "The following cells will launch a training session for a policy\n",
    "The first cell will only load the necessary assets and set everything up, while the second one will load tensorboard to visualize useful data about the ongoing training.\n",
    "\n",
    "> NOTE: this directory contains a pre-trained policy \"stand\". You can skip the training cells if you don't need anything specific, and jump to the testing section\n",
    "\n",
    "## Parameters\n",
    "- You can set the name of the policy being trained by assignin it to the \"run\" variable.\n",
    "- You can set the number of checkpoints that will be saved, changing the number within \"checkpoint_callback\"\n",
    "- You can adjust learning rate, entropy coefficient, clip range and the rest of the hyperparameters on the last fiew lines of the notebook\n",
    "\n",
    "## The rewad function\n",
    "The reward function is defined in another file, and is crucial to the success of the experiment. In this case, I have defined 5 different rewards/penalties to define a good standing behaviour:\n",
    "- _Uprightness_: this metric should measure the posture of the gait, and should encourage it to stand upright. It is measured through roll and pitch\n",
    "- _Height_: the closer the agent is to a target height set by the user, the bigger the reward it receives\n",
    "- _Vertical velocity penalty_: any sudden and fast movement on the z-axis is heavily penalized, to encourage stillness\n",
    "- _Joint deviation penalty_: the more the position of each joint strays from a set position (homing positions), the heavier the penalty is. This ecnourages the agent to stick to a predefined resting pose\n",
    "-  _Action sparsity reward_: this metric rewards the agent for small actions, and should discourage ample movements\n",
    "\n",
    "Each reward /penalty is linearly combined withh the others with a weight that highlight its importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40900bb-0801-4791-89c1-439a8190c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# ========= CONFIG ==========\n",
    "TOTAL_STEPS = 4_000_000\n",
    "run = \"stand\"\n",
    "log_dir = f\"./logs/{run}\"\n",
    "\n",
    "def clipped_linear_schedule(initial_value, min_value=1e-5):\n",
    "    def schedule(progress_remaining):\n",
    "        return max(progress_remaining * initial_value, min_value)\n",
    "    return schedule\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=TOTAL_STEPS // 10,\n",
    "    save_path=f\"{run}_checkpoints\",\n",
    "    name_prefix=f\"ppo_{run}\"\n",
    ")\n",
    "\n",
    "# ========= ENV ==========\n",
    "env = SpotmicroEnv(\n",
    "    use_gui=False,\n",
    "    reward_fn=reward_function, \n",
    "    reward_state=RewardState(), \n",
    "    dest_save_file=f\"{run}.pkl\"\n",
    ")\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# ========= MODEL ==========\n",
    "model = PPO(\n",
    "    \"MlpPolicy\", \n",
    "    env,\n",
    "    verbose=0,   # no default printouts\n",
    "    learning_rate=clipped_linear_schedule(3e-4),\n",
    "    ent_coef=0.002,\n",
    "    clip_range=0.1,\n",
    "    tensorboard_log=log_dir,\n",
    ")\n",
    "\n",
    "# Custom logger: ONLY csv + tensorboard (no stdout table)\n",
    "new_logger = configure(log_dir, [\"csv\", \"tensorboard\"])\n",
    "model.set_logger(new_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2474099-7ae6-42dc-88ae-4830f0ce9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfac21d-4a2e-408e-9331-f5fd3b9cb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========= TRAIN ==========\n",
    "model.learn(\n",
    "    total_timesteps=TOTAL_STEPS,\n",
    "    reset_num_timesteps=False,\n",
    "    callback=checkpoint_callback\n",
    ")\n",
    "model.save(f\"policies/ppo_{run}\")\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa9b9cc4-083a-4059-b544-7d9fe62d21bc",
   "metadata": {},
   "source": [
    "# Testing\n",
    "The following cells allow to test the policy we have just trained. All we have to do is assign the name of the policy we have trained to the \"policy\" variable.\n",
    "You can then run the second to last cell any times you want, and observe a single episode until termination. When you are done, execute the last cell to clean everything up.\n",
    "\n",
    "> If in any case there seems to be some sort of weird error, try to reload the kernel of this jupyter notebook first (pybullet is kind of messy in its cleanup phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaecb925-6799-46db-8d94-97e94dd631cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'spawn_height'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SpotmicroEnv(\n\u001b[32m      6\u001b[39m         use_gui=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m         reward_fn=reward_function,\n\u001b[32m      8\u001b[39m         reward_state=RewardState(),\n\u001b[32m      9\u001b[39m         src_save_file=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pkl\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# DummyVecEnv wrapper\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m raw_env = \u001b[43mDummyVecEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmake_env\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# === Load VecNormalize stats ===\u001b[39;00m\n\u001b[32m     16\u001b[39m eval_env = VecNormalize.load(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_vecnormalize.pkl\u001b[39m\u001b[33m\"\u001b[39m, raw_env)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:31\u001b[39m, in \u001b[36mDummyVecEnv.__init__\u001b[39m\u001b[34m(self, env_fns)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: \u001b[38;5;28mlist\u001b[39m[Callable[[], gym.Env]]):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28mself\u001b[39m.envs = [_patch_env(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env.unwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.envs])) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.envs):\n\u001b[32m     33\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     34\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minstead of creating different objects. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m         )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mmake_env\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_env\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSpotmicroEnv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_gui\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreward_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreward_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRewardState\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_save_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpolicy\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.pkl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/policy_training/SpotmicroEnv.py:93\u001b[39m, in \u001b[36mSpotmicroEnv.__init__\u001b[39m\u001b[34m(self, envConfig, agentConfig, terrainConfig, use_gui, reward_fn, reward_state, dest_save_file, src_save_file, writer)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m._terrain.generate()\n\u001b[32m     84\u001b[39m pybullet.changeDynamics(\n\u001b[32m     85\u001b[39m     bodyUniqueId=\u001b[38;5;28mself\u001b[39m._terrain.terrain_id,\n\u001b[32m     86\u001b[39m     linkIndex=-\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m     physicsClientId=\u001b[38;5;28mself\u001b[39m.physics_client\n\u001b[32m     92\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._agent = \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mphysics_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43magentConfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ACT_SPACE_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m._dest_save = dest_save_file\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dest_save \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aea_spot_micro/policy_training/Agent.py:87\u001b[39m, in \u001b[36mAgent.__init__\u001b[39m\u001b[34m(self, physics_client, config, action_space_size)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m._action_space_size = action_space_size\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# --- State ---\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m._state = AgentState(\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     base_position=np.array([\u001b[32m0.0\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn_height\u001b[49m]),\n\u001b[32m     88\u001b[39m     base_orientation=pybullet.getQuaternionFromEuler([\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m._config.homing_pitch, np.pi]),\n\u001b[32m     89\u001b[39m )\n\u001b[32m     90\u001b[39m \u001b[38;5;28mself\u001b[39m._action = np.zeros(\u001b[38;5;28mself\u001b[39m._action_space_size, dtype=np.float32)\n\u001b[32m     91\u001b[39m \u001b[38;5;28mself\u001b[39m._previous_action = np.zeros(\u001b[38;5;28mself\u001b[39m._action_space_size, dtype=np.float32)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Config' object has no attribute 'spawn_height'"
     ]
    }
   ],
   "source": [
    "policy = \"stand\"\n",
    "\n",
    "# === Build raw env ===\n",
    "def make_env():\n",
    "    return SpotmicroEnv(\n",
    "        use_gui=True,\n",
    "        reward_fn=reward_function,\n",
    "        reward_state=RewardState(),\n",
    "        src_save_file=f\"{policy}.pkl\",\n",
    "    )\n",
    "\n",
    "# DummyVecEnv wrapper\n",
    "raw_env = DummyVecEnv([make_env])\n",
    "\n",
    "# === Load VecNormalize stats ===\n",
    "eval_env = VecNormalize.load(f\"{policy}_vecnormalize.pkl\", raw_env)\n",
    "\n",
    "# Very important: disable training updates during evaluation\n",
    "eval_env.training = False\n",
    "eval_env.norm_reward = False\n",
    "\n",
    "# === Load model ===\n",
    "model = PPO.load(f\"stand_checkpoints/ppo_{policy}_5000000_steps.zip\")\n",
    "\n",
    "print(\"Loaded policy and VecNormalize stats\")\n",
    "\n",
    "# === Run rollout ===\n",
    "obs = eval_env.reset()\n",
    "for _ in range(2000):  # run some steps\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = eval_env.step(action)\n",
    "    eval_env.render()\n",
    "    if done:\n",
    "        obs = eval_env.reset()\n",
    "\n",
    "    spot_env = eval_env.venv.envs[0]\n",
    "    print(spot_env.agent.action)\n",
    "\n",
    "    time.sleep(1/60.)\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91375798-c6f6-4895-b7ca-c4a458e7b9d7",
   "metadata": {},
   "source": [
    "# What is next?\n",
    "The next important step towards walking is convincing the policy to move at all. It is not a trivial task, since it involves designing a reward function that makes moving more attractive than both standing still and falling flat. This task is explored in the notebook inside the \"shuffling\" directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
