{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb68d890-186f-43ef-8c85-3bf3e20712fe",
   "metadata": {},
   "source": [
    "# Shuffling behaviour\n",
    "The purpose of this notebook is to train to move a policy that has learned to _stand_. \n",
    "It is the first step towards walking, and usually manifests with a policy that moves around dragging its feet (-> shuffling around)\n",
    "Again, we will provide a training and an evaluation section, along with the reward function and the \"stand\" policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23fef0-3456-4a40-bdb2-be99c52fc965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Start from the current working directory (where notebook is)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Go two levels up (to the \"grandparent\")\n",
    "grandparent_dir = os.path.abspath(os.path.join(cwd, \"..\", \"..\"))\n",
    "\n",
    "# Add to sys.path if not already there\n",
    "if grandparent_dir not in sys.path:\n",
    "    sys.path.insert(0, grandparent_dir)\n",
    "\n",
    "from SpotmicroEnv import SpotmicroEnv\n",
    "from reward_function import reward_function, RewardState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7bb15c-4153-4613-839a-0b1debcc3423",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ac4c2-12db-432a-98b4-7602eb6e40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# ========= CONFIG ==========\n",
    "TOTAL_STEPS = 8_000_000\n",
    "run = \"shuffle\"\n",
    "base=\"stand\"\n",
    "\n",
    "log_dir = f\"./logs/{run}\"\n",
    "\n",
    "def clipped_linear_schedule(initial_value, min_value=1e-5):\n",
    "    def schedule(progress_remaining):\n",
    "        return max(progress_remaining * initial_value, min_value)\n",
    "    return schedule\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=TOTAL_STEPS // 10,\n",
    "    save_path=f\"{run}_checkpoints\",\n",
    "    name_prefix=f\"ppo_{run}\"\n",
    ")\n",
    "\n",
    "# ========= ENV ==========\n",
    "env = SpotmicroEnv(\n",
    "    use_gui=False,\n",
    "    reward_fn=reward_function, \n",
    "    reward_state=RewardState(),\n",
    "    src_save_file=f\"{base}.pkl\"\n",
    "    dest_save_file=f\"{run}.pkl\"\n",
    ")\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# ========= MODEL ==========\n",
    "model = PPO.load(f\"ppo_{base}\")\n",
    "model.set_env(env)\n",
    "model.tensorboard_log = log_dir\n",
    "\n",
    "# Custom logger: ONLY csv + tensorboard (no stdout table)\n",
    "new_logger = configure(log_dir, [\"csv\", \"tensorboard\"])\n",
    "model.set_logger(new_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79693176-28e2-4166-b359-8e5f1d2a36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=TOTAL_STEPS,\n",
    "    reset_num_timesteps=False,\n",
    "    callback=checkpoint_callback\n",
    ")\n",
    "model.save(f\"ppo_{run}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
