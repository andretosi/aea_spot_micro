{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e09453-83ba-40a2-b3bb-a7cf2404c8b4",
   "metadata": {},
   "source": [
    "# Standing Behaviour\n",
    "> This directory contains all the building blocks required to achieve the most basic learnable behaviour: standing still.  \n",
    "> \n",
    "> âš ï¸ **IMPORTANT WARNING** âš ï¸  \n",
    "> This notebook, together with the contents of the entire directory, is intended to serve as a tutorial and documentation of the results achieved so far.  \n",
    "> It should **NOT** be modified.  \n",
    "> \n",
    "> __To experiment with this task, please create a copy of this directory and make your changes there.__\n",
    "\n",
    "## Contents\n",
    "This directory includes:\n",
    "- This notebook  \n",
    "- The file `reward_function.py`, which defines a reward function tailored for achieving standing behaviour with deep reinforcement learning  \n",
    "- `ppo_stand.zip`, a pretrained policy that demonstrates standing behaviour  \n",
    "- Three config files (`env`, `agent`, and `terrain` configs) that specify all parameters for this experiment. In particular, note the definition of the **home positions** of each joint in `agentConfig.yaml`, since these determine the pose the robot assumes when resting  \n",
    "- `stand.pkl`, a state file produced at the end of a training session. It is used by the environment to store necessary data such as the total number of training steps  \n",
    "\n",
    "**Note:** The file `SpotmicroEnv.py` defines the custom training environment. For the notebook to work, it must be located in the *grandparent directory* of this one.\n",
    "\n",
    "## Use\n",
    "This notebook, along with the directory contents, demonstrates the basic workflow of training and testing a simple policy.  \n",
    "It also serves as documentation, since this policy will be used as a foundation for later experiments.\n",
    "\n",
    "The notebook is organized as follows:\n",
    "- The first cell (imports) must be executed every time; it loads almost all dependencies  \n",
    "- The first section covers training a custom policy.  \n",
    "  - To experiment, copy this directory elsewhere and adjust the reward function or hyperparameters there  \n",
    "  - Otherwise, you can stick to the provided base policy and skip directly to testing  \n",
    "- The final section covers testing: exploring the results and analyzing the learned policy  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b47e99-3d92-4f7a-b231-00dbf14f381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Apr  4 2025 18:56:19\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Start from the current working directory (where notebook is)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Go two levels up (to the \"grandparent\")\n",
    "grandparent_dir = os.path.abspath(os.path.join(cwd, \"..\", \"..\"))\n",
    "\n",
    "# Add to sys.path if not already there\n",
    "if grandparent_dir not in sys.path:\n",
    "    sys.path.insert(0, grandparent_dir)\n",
    "\n",
    "from SpotmicroEnv import SpotmicroEnv\n",
    "from reward_function import reward_function, RewardState\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a57400-cb70-4ea5-a613-4347a0058eb8",
   "metadata": {},
   "source": [
    "# Training\n",
    "The following cells will launch a training session for a policy\n",
    "The first cell will only load the necessary assets and set everything up, while the second one will load tensorboard to visualize useful data about the ongoing training.\n",
    "\n",
    "> NOTE: this directory contains a pre-trained policy \"stand\". You can skip the training cells if you don't need anything specific, and jump to the testing section\n",
    "\n",
    "## Parameters\n",
    "- You can set the name of the policy being trained by assignin it to the \"run\" variable.\n",
    "- You can set the number of checkpoints that will be saved, changing the number within \"checkpoint_callback\"\n",
    "- You can adjust learning rate, entropy coefficient, clip range and the rest of the hyperparameters on the last fiew lines of the notebook\n",
    "\n",
    "## The rewad function\n",
    "The reward function is defined in another file, and is crucial to the success of the experiment. In this case, a single metric is sufficient to define the desired behaviour. The agent is given a reward inversely proportional to the average magnitude of each action: the closer the action average is to 0 (and to the homing position) the higher the reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40900bb-0801-4791-89c1-439a8190c2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "front_left_leg_link_coverb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "front_right_leg_link_coverb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "rear_left_leg_link_coverb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:\n",
      "rear_right_leg_link_cover"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/aea_spot_micro/policy_training/SpotmicroEnv.py:113: UserWarning: File 'stand.pkl' already exists and will be overwritten.\n",
      "  warnings.warn(f\"File '{self._dest_save}' already exists and will be overwritten.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# ========= CONFIG ==========\n",
    "TOTAL_STEPS = 5_000_000\n",
    "run = \"stand\"\n",
    "log_dir = f\"./logs/{run}\"\n",
    "\n",
    "def clipped_linear_schedule(initial_value, min_value=1e-5):\n",
    "    def schedule(progress_remaining):\n",
    "        return max(progress_remaining * initial_value, min_value)\n",
    "    return schedule\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=TOTAL_STEPS // 10,\n",
    "    save_path=f\"{run}_checkpoints\",\n",
    "    name_prefix=f\"ppo_{run}\"\n",
    ")\n",
    "\n",
    "# ========= ENV ==========\n",
    "env = SpotmicroEnv(\n",
    "    use_gui=False,\n",
    "    reward_fn=reward_function, \n",
    "    reward_state=RewardState(), \n",
    "    dest_save_file=f\"{run}.pkl\"\n",
    ")\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# ========= MODEL ==========\n",
    "model = PPO(\n",
    "    \"MlpPolicy\", \n",
    "    env,\n",
    "    verbose=0,   # no default printouts\n",
    "    learning_rate=clipped_linear_schedule(3e-4),\n",
    "    ent_coef=0.002,\n",
    "    clip_range=0.1,\n",
    "    tensorboard_log=log_dir,\n",
    ")\n",
    "\n",
    "# Custom logger: ONLY csv + tensorboard (no stdout table)\n",
    "new_logger = configure(log_dir, [\"csv\", \"tensorboard\"])\n",
    "model.set_logger(new_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2474099-7ae6-42dc-88ae-4830f0ce9ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-552aac896b0ca27e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-552aac896b0ca27e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dfac21d-4a2e-408e-9331-f5fd3b9cb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========= TRAIN ==========\n",
    "model.learn(\n",
    "    total_timesteps=TOTAL_STEPS,\n",
    "    reset_num_timesteps=False,\n",
    "    callback=checkpoint_callback\n",
    ")\n",
    "model.save(f\"policies/ppo_{run}\")\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa9b9cc4-083a-4059-b544-7d9fe62d21bc",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "The following cells allow to test the policy we have just trained. All we have to do is assign the name of the policy we have trained to the \"policy\" variable.\n",
    "You can then run the second to last cell any times you want, and observe a single episode until termination. When you are done, execute the last cell to clean everything up.\n",
    "\n",
    "> If in any case there seems to be some sort of weird error, try to reload the kernel of this jupyter notebook first (pybullet is kind of messy in its cleanup phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaecb925-6799-46db-8d94-97e94dd631cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env steps: 5001227\n",
      "Terminated\n",
      "dict_keys(['mean_abs_tau', 'rms_tau', 'max_tau', 'total_abs_torque', 'total_work', 'per_joint_mean', 'per_joint_max', 'episode_steps'])\n",
      "Total work: 1.528600918588716\n",
      "RMS torque: 1.449903302422067\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "policy = \"stand\"\n",
    "\n",
    "env = SpotmicroEnv(\n",
    "    use_gui=True, \n",
    "    reward_fn=reward_function,\n",
    "    reward_state=RewardState(),\n",
    "    src_save_file=f\"{policy}.pkl\"\n",
    "    )\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# === Load model ===\n",
    "model = PPO.load(f\"ppo_{policy}\")\n",
    "print(f\"env steps: {env.num_steps}\")\n",
    "\n",
    "# === Run rollout ===\n",
    "for _ in range(3001):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"Terminated\")\n",
    "        env.plot_reward_components()  # ðŸ‘ˆ plot per episode\n",
    "        obs, _ = env.reset()\n",
    "    time.sleep(1/60)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91375798-c6f6-4895-b7ca-c4a458e7b9d7",
   "metadata": {},
   "source": [
    "# What is next?\n",
    "The next important step towards walking is convincing the policy to move at all. It is not a trivial task, since it involves designing a reward function that makes moving more attractive than both standing still and falling flat. This task is explored in the notebook inside the \"shuffling\" directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
